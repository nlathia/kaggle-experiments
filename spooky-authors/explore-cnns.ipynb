{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "TRAIN = 'data/train.csv'\n",
    "TEST = 'data/test.csv'\n",
    "SAMPLE = 'data/sample_submission.csv'\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NB_WORDS = 20000\n",
    "\n",
    "train = pd.read_csv(TRAIN)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15663,), (15663,), (3916,), (3916,))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train.text\n",
    "y = train.author\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23891 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['EAP'],\n",
       "       ['EAP'],\n",
       "       ['HPL'],\n",
       "       ['MWS'],\n",
       "       ['MWS'],\n",
       "       ['HPL'],\n",
       "       ['EAP'],\n",
       "       ['MWS'],\n",
       "       ['EAP'],\n",
       "       ['HPL']], dtype=object)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.values.reshape(-1, 1)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 2, 2, 1, 0, 2, 0, 1])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "train_labels = le.fit_transform(y_train)\n",
    "test_labels = le.transform(y_test)\n",
    "\n",
    "train_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "train_labels = mlb.fit_transform(train_labels.reshape(-1, 1))\n",
    "test_labels = mlb.transform(test_labels.reshape(-1, 1))\n",
    "\n",
    "train_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15663, 1000), (3916, 1000))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "train_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15663, 3), (3916, 3))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_8 (Embedding)          (None, 1000, 50)      1194600     embedding_input_8[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)              (None, 50000)         0           embedding_8[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_15 (Dense)                 (None, 100)           5000100     flatten_8[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)             (None, 100)           0           dense_15[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_16 (Dense)                 (None, 3)             303         dropout_10[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 6,195,003\n",
      "Trainable params: 6,195,003\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=len(word_index) + 1, output_dim=50, input_length=MAX_SEQUENCE_LENGTH),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.7),\n",
    "    Dense(3, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/10\n",
      "15663/15663 [==============================] - 40s - loss: 0.6528 - acc: 0.6609 - val_loss: 0.6242 - val_acc: 0.6667\n",
      "Epoch 2/10\n",
      "15663/15663 [==============================] - 31s - loss: 0.5685 - acc: 0.7117 - val_loss: 0.4737 - val_acc: 0.7580\n",
      "Epoch 3/10\n",
      "15663/15663 [==============================] - 30s - loss: 0.3359 - acc: 0.8510 - val_loss: 0.2745 - val_acc: 0.8853\n",
      "Epoch 4/10\n",
      "15663/15663 [==============================] - 30s - loss: 0.1317 - acc: 0.9537 - val_loss: 0.2800 - val_acc: 0.8910\n",
      "Epoch 5/10\n",
      "15663/15663 [==============================] - 37s - loss: 0.0606 - acc: 0.9817 - val_loss: 0.3217 - val_acc: 0.8898\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x112a4de80>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=1)\n",
    "\n",
    "model.fit(train_data, train_labels, validation_data=(test_data, test_labels), batch_size=64, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_9 (Embedding)          (None, 1000, 50)      1194600     embedding_input_9[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)             (None, 1000, 50)      0           embedding_9[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_2 (Convolution1D)  (None, 1000, 64)      16064       dropout_11[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)             (None, 1000, 64)      0           convolution1d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_2 (MaxPooling1D)    (None, 500, 64)       0           dropout_12[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)              (None, 32000)         0           maxpooling1d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_17 (Dense)                 (None, 100)           3200100     flatten_9[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)             (None, 100)           0           dense_17[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_18 (Dense)                 (None, 3)             303         dropout_13[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 4,411,067\n",
      "Trainable params: 4,411,067\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras.layers.pooling import MaxPooling1D\n",
    "\n",
    "model2 = Sequential([\n",
    "    Embedding(input_dim=len(word_index) + 1, output_dim=50, input_length=MAX_SEQUENCE_LENGTH),\n",
    "    Dropout(0.2),\n",
    "    Convolution1D(64, 5, activation='relu', border_mode='same'),\n",
    "    Dropout(0.2),\n",
    "    MaxPooling1D(),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.7),\n",
    "    Dense(3, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model2.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/10\n",
      "15663/15663 [==============================] - 277s - loss: 0.5732 - acc: 0.7035 - val_loss: 0.3744 - val_acc: 0.8527\n",
      "Epoch 2/10\n",
      "15663/15663 [==============================] - 297s - loss: 0.2919 - acc: 0.8841 - val_loss: 0.2642 - val_acc: 0.8899\n",
      "Epoch 3/10\n",
      "15663/15663 [==============================] - 302s - loss: 0.1633 - acc: 0.9416 - val_loss: 0.2721 - val_acc: 0.8858\n",
      "Epoch 4/10\n",
      "15663/15663 [==============================] - 309s - loss: 0.1039 - acc: 0.9634 - val_loss: 0.3277 - val_acc: 0.8773\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1219d7cc0>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=1)\n",
    "\n",
    "model2.fit(train_data, train_labels, validation_data=(test_data, test_labels), batch_size=64, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
